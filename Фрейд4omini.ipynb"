{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AnIKoadmhPhIsKmlEx2fNvR27MovURFc","timestamp":1737788368369}],"mount_file_id":"1Xaw_3u6SI77iZsH5XzG4kF1IH88I3hkj","authorship_tag":"ABX9TyNiVvkHQGj5wmhh7ZjAgey1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"EV8FBUcl20P8","executionInfo":{"status":"ok","timestamp":1737878793792,"user_tz":-240,"elapsed":8587,"user":{"displayName":"Сергей","userId":"03696252463038716506"}},"outputId":"3cd79f32-11ab-4770-ec93-8c68bd8f348b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.5)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"]}],"source":["!pip install openai"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ccM0dnq_-MG8","executionInfo":{"status":"ok","timestamp":1737878804357,"user_tz":-240,"elapsed":10586,"user":{"displayName":"Сергей","userId":"03696252463038716506"}},"outputId":"dde90e9e-7670-4361-d915-e76a6dfc1ca4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting telebot\n","  Downloading telebot-0.0.5-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Collecting pyTelegramBotAPI (from telebot)\n","  Downloading pytelegrambotapi-4.26.0-py3-none-any.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from telebot) (2.32.3)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->telebot) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->telebot) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->telebot) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->telebot) (2024.12.14)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Downloading telebot-0.0.5-py3-none-any.whl (4.8 kB)\n","Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytelegrambotapi-4.26.0-py3-none-any.whl (270 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.5/270.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu, pyTelegramBotAPI, telebot\n","Successfully installed faiss-cpu-1.9.0.post1 pyTelegramBotAPI-4.26.0 telebot-0.0.5\n"]}],"source":["!pip install  telebot sentence-transformers  faiss-cpu numpy"]},{"cell_type":"code","source":["import os\n","from datetime import datetime\n","import sys\n","import re\n","import logging\n","import threading\n","import numpy as np\n","import faiss\n","import openai\n","\n","from telebot import TeleBot, types\n","from sentence_transformers import SentenceTransformer\n","\n","from collections import deque\n","\n","from sklearn.preprocessing import normalize\n","from openai import OpenAI\n","\n","# Инициализация клиента OpenAI\n","client = OpenAI(key)\n","import json\n","import requests\n","\n","# Настройка логирования\n","logging.basicConfig(\n","    level=logging.INFO,  # Уровень логирования (INFO, DEBUG, WARNING, ERROR, CRITICAL)\n","    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Формат сообщений\n","    handlers=[logging.StreamHandler()]  # Вывод в консоль\n",")\n","\n","\n","\n","\n","\n","# Загрузка моделей\n","embedding_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')  # Модель для создания эмбеддингов\n","\n","\n","# Путь к книге\n","book_path = \"/content/drive/MyDrive/Фрейд Зигмунд. Толкование сновидений - royallib.ru.txt\"\n","\n","# Путь к файлу с эмбеддингами\n","embeddings_file = \"/content/drive/MyDrive/embeddings.npy\"\n","# Загрузка эмбеддингов из файла\n","logging.info(\"Загрузка эмбеддингов из файла...\")\n","embeddings = np.load(embeddings_file)\n","\n","\n","\n","\n","# Функция для загрузки текста книги\n","def load_book_text(file_path, encoding=\"windows-1251\"):\n","    \"\"\"Загружает текст книги из файла.\"\"\"\n","    try:\n","        with open(file_path, \"r\", encoding=encoding) as file:\n","            return file.read()\n","    except Exception as e:\n","        logging.error(f\"Ошибка при загрузке книги: {e}\")\n","        raise\n","# Загрузка текста книги\n","book_text = load_book_text(book_path)\n","\n","\n","# Инициализация истории сообщений\n","user_text_messages = {}\n","\n","\n","\n","def split_text_into_chunks(text, chunk_size=500, overlap=100):\n","    \"\"\"\n","    Разделяет текст на отрывки, сохраняя целостность предложений.\n","\n","    Параметры:\n","        text (str): Исходный текст.\n","        chunk_size (int): Максимальный размер чанка в символах.\n","        overlap (int): Количество символов перекрытия между чанками.\n","\n","    Возвращает:\n","        list: Список чанков.\n","    \"\"\"\n","    # Удаляем лишние пробелы и специальные символы\n","    text = re.sub(r'\\s+', ' ', text).strip()  # Заменяем множественные пробелы на один\n","    text = text.replace('\\xa0', ' ')  # Заменяем неразрывные пробелы на обычные\n","\n","    # Разбиваем текст на предложения\n","    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Разделяем по точкам, восклицательным и вопросительным знакам\n","\n","    chunks = []\n","    current_chunk = \"\"\n","    current_length = 0\n","\n","    for sentence in sentences:\n","        # Если добавление текущего предложения не превышает chunk_size, добавляем его в текущий чанк\n","        if current_length + len(sentence) <= chunk_size:\n","            current_chunk += sentence + \" \"\n","            current_length += len(sentence) + 1\n","        else:\n","            # Если чанк заполнен, добавляем его в список чанков\n","            if current_chunk:\n","                chunks.append(current_chunk.strip())\n","\n","            # Начинаем новый чанк с перекрытием\n","            if overlap > 0 and chunks:\n","                previous_chunk = chunks[-1]\n","                overlap_start = max(0, len(previous_chunk) - overlap)\n","                current_chunk = previous_chunk[overlap_start:] + \" \" + sentence + \" \"\n","            else:\n","                current_chunk = sentence + \" \"\n","\n","            current_length = len(current_chunk)\n","\n","    # Добавляем последний чанк, если он не пустой\n","    if current_chunk:\n","        chunks.append(current_chunk.strip())\n","\n","    return chunks\n","\n","# Разбиение текста на чанки с одинаковыми параметрами\n","text_chunks = split_text_into_chunks(book_text, chunk_size=500, overlap=100)\n","print(f\"Количество текстовых чанков: {len(text_chunks)}\")\n","\n","\n","\n","\n","def create_faiss_index(text_chunks, embeddings_file=\"/content/drive/MyDrive/embeddings.npy\"):\n","    \"\"\"\n","    Создает индекс FAISS с кэшированием эмбеддингов.\n","    \"\"\"\n","    try:\n","        # Проверяем, что text_chunks не пуст\n","        if not text_chunks:\n","            logging.error(\"Список text_chunks пуст. Невозможно создать индекс.\")\n","            return None, None\n","\n","        # Загружаем эмбеддинги из кэша или создаём новые\n","        if os.path.exists(embeddings_file):\n","            logging.info(\"Загрузка эмбеддингов из кэша...\")\n","            embeddings = np.load(embeddings_file)\n","            # Нормализация эмбеддингов\n","            embeddings = normalize(embeddings, norm='l2', axis=1)\n","\n","        # Проверка нормализации\n","            norms = np.linalg.norm(embeddings, axis=1)\n","            print(f\"Минимальная норма после нормализации: {np.min(norms)}\")\n","            print(f\"Максимальная норма после нормализации: {np.max(norms)}\")\n","        else:\n","            logging.info(\"Создание новых эмбеддингов...\")\n","            embeddings = embedding_model.encode(text_chunks)\n","            np.save(embeddings_file, embeddings)\n","\n","        # Проверяем, что эмбеддинги созданы корректно\n","        if embeddings.shape[0] == 0:\n","            logging.error(\"Эмбеддинги не были созданы. Проверьте модель и данные.\")\n","            return None, None\n","\n","        # Получаем размерность эмбеддингов\n","        dimension = embeddings.shape[1]\n","\n","        # Создаём индекс FAISS\n","        index = faiss.IndexFlatL2(dimension)  # Используем L2 расстояние\n","        embeddings_array = np.array(embeddings, dtype=np.float32)  # Преобразуем в массив numpy\n","        index.add(embeddings_array)  # Добавляем эмбеддинги в индекс\n","\n","        print(f\"Индекс FAISS успешно создан. Размерность: {dimension}, количество элементов: {index.ntotal}.\")\n","\n","        return index, text_chunks\n","\n","    except Exception as e:\n","        logging.error(f\"Ошибка при создании индекса FAISS: {e}\")\n","        return None, None\n","# Создание индекса FAISS\n","index, text_chunks = create_faiss_index(text_chunks, embeddings_file)\n","\n","\n","\n","def search_relevant_texts(query, index, text_data, model, top_k=5, threshold=0):\n","    \"\"\"\n","    Ищет релевантные отрывки текста по запросу.\n","\n","    Параметры:\n","        query (str): Запрос пользователя.\n","        index (faiss.Index): Индекс FAISS.\n","        text_data (list): Список текстовых чанков.\n","        model: Модель для создания эмбеддингов.\n","        top_k (int): Количество возвращаемых отрывков.\n","        threshold (float): Порог релевантности (косинусное сходство).\n","\n","    Возвращает:\n","        list: Релевантные отрывки текста.\n","    \"\"\"\n","    try:\n","        # Проверяем, что индекс FAISS не пуст\n","        if index.ntotal == 0:\n","            logging.error(\"Индекс FAISS пуст. Невозможно выполнить поиск.\")\n","            return []\n","                # Проверяем соответствие индекса и данных\n","        if index.ntotal != len(text_data):\n","            logging.error(\"Размер индекса FAISS не соответствует размеру text_data.\")\n","            return []\n","        # Преобразуем запрос в эмбеддинг\n","        query_embedding = model.encode([query])\n","        query_embedding = np.array(query_embedding, dtype=np.float32)  # Убедимся, что тип данных правильный\n","\n","        # Нормализуем эмбеддинг запроса\n","        faiss.normalize_L2(query_embedding)\n","\n","        # Ищем ближайшие соседи в индексе\n","        distances, indices = index.search(query_embedding, top_k)\n","              # Фильтруем результаты по порогу релевантности\n","        relevant_chunks = []\n","        for i, distance in zip(indices[0], distances[0]):\n","            try:\n","                # Проверяем, что индекс i находится в пределах text_data\n","                if 0 <= i < len(text_data):\n","                    # Проверяем, что расстояние соответствует порогу\n","                    if 1 - distance > threshold:  # Косинусное сходство = 1 - расстояние\n","                        relevant_chunks.append(text_data[i])\n","                else:\n","                    logging.warning(f\"Индекс {i} выходит за пределы text_data. Пропуск.\")\n","            except Exception as e:\n","                logging.error(f\"Ошибка при обработке индекса {i}: {e}\")\n","\n","\n","\n","        # Логируем результат\n","        print(f\"Найдено {len(relevant_chunks)} релевантных отрывков.\")\n","        print(f\"Расстояния: {distances}\")\n","        print(f\"Индексы: {indices}\")\n","        print(\"    контекст    :\", relevant_chunks)\n","\n","        # Возвращаем релевантные отрывки\n","        return relevant_chunks\n","\n","    except Exception as e:\n","        logging.error(f\"Ошибка при поиске релевантных текстов: {e}\")\n","        return []\n","\n","from langchain_core.messages import HumanMessage, SystemMessage\n","\n","def generate_gpt4_response(query, context, user_text_messages, user_id):\n","    try:\n","        # Инициализация истории сообщений для пользователя\n","        if user_id not in user_text_messages:\n","            user_text_messages[user_id] = deque(maxlen=6)  # Ограничиваем историю 6 сообщениями\n","\n","        # Добавляем новый запрос пользователя в историю\n","        user_text_messages[user_id].append({\"role\": \"user\", \"content\": query})\n","\n","        # Формируем системное сообщение с контекстом\n","        system_message = {\"role\": \"system\", \"content\": f\"Ты психолог, специализирующийся на толковании сновидений по Фрейду. Отвечай на основе книги 'Толкование сновидений' по Фрейду: {context}\"}\n","\n","        # Формируем список сообщений для модели\n","        messages = [system_message] + list(user_text_messages[user_id])\n","\n","        # Генерация ответа через OpenAI API\n","        response = client.chat.completions.create(\n","            model=\"gpt-4o-mini\",  # Укажите здесь модель, которую вы хотите использовать\n","            messages=messages\n","        )\n","\n","        # Добавляем ответ ассистента в историю сообщений\n","        user_text_messages[user_id].append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n","\n","        return response.choices[0].message.content\n","\n","    except Exception as e:\n","        # Логируем ошибку, если что-то пошло не так\n","        print(f\"Ошибка при генерации ответа через OpenAI: {e}\")\n","        return \"Произошла ошибка при обработке вашего запроса. Пожалуйста, попробуйте позже.\"\n","# Токен вашего бота\n","TELEGRAM_BOT_TOKEN = \"token\"\n","\n","# Инициализация бота\n","bot = TeleBot(TELEGRAM_BOT_TOKEN)\n","\n","# Обработчик команды /start\n","@bot.message_handler(commands=[\"start\"])\n","def start(message: types.Message):\n","    bot.send_message(message.chat.id, \"Привет! Я бот-психолог, специализирующийся на толковании сновидений по Фрейду. Задайте ваш вопрос.\")\n","\n","\n","@bot.message_handler(func=lambda message: True)\n","def handle_message(message: types.Message):\n","    query = message.text\n","    user_id = message.chat.id\n","\n","    try:\n","        # Поиск релевантных отрывков\n","        relevant_texts = search_relevant_texts(query, index, text_chunks, embedding_model)\n","        context = \" \".join(relevant_texts)\n","\n","        # Генерация ответа через OpenAI API\n","        bot_response = generate_gpt4_response(query, context, user_text_messages, user_id)\n","\n","        # Отправка ответа пользователю\n","        bot.send_message(user_id, bot_response)\n","\n","    except Exception as e:\n","        logging.error(f\"Ошибка: {e}\")\n","        bot.send_message(user_id, \"Произошла ошибка при обработке вашего запроса. Пожалуйста, попробуйте позже.\")\n","\n","# Запуск бота\n","if __name__ == \"__main__\":\n","    logging.info(\"Бот запущен.\")\n","    bot.infinity_polling()"],"metadata":{"id":"2rkjvV6n-jxA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a1662db-66e3-4210-ffac-9699826c087a","executionInfo":{"status":"ok","timestamp":1737879008969,"user_tz":-240,"elapsed":28509,"user":{"displayName":"Сергей","userId":"03696252463038716506"}},"collapsed":true},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Количество текстовых чанков: 3768\n","Минимальная норма после нормализации: 0.9999998211860657\n","Максимальная норма после нормализации: 1.0000001192092896\n","Индекс FAISS успешно создан. Размерность: 512, количество элементов: 3768.\n","Найдено 0 релевантных отрывков.\n","Расстояния: [[1.5965252 1.6174941 1.6223649 1.6444772 1.6494138]]\n","Индексы: [[3298 2927  410 3543 1004]]\n","    контекст    : []\n","Ошибка при генерации ответа через OpenAI: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"]},{"output_type":"stream","name":"stderr","text":["2025-01-26 08:10:08,840 (__init__.py:1121 MainThread) ERROR - TeleBot: \"Infinity polling: polling exited\"\n","ERROR:TeleBot:Infinity polling: polling exited\n","2025-01-26 08:10:08,843 (__init__.py:1123 MainThread) ERROR - TeleBot: \"Break infinity polling\"\n","ERROR:TeleBot:Break infinity polling\n"]}]},{"cell_type":"code","source":["models = client.models.list()\n","for model in models.data:\n","    print(model.id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXFpKbIUc6YX","executionInfo":{"status":"ok","timestamp":1737878933314,"user_tz":-240,"elapsed":583,"user":{"displayName":"Сергей","userId":"03696252463038716506"}},"outputId":"95226833-5d0a-456c-d2d5-1c3956b299e3","collapsed":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["dall-e-2\n","gpt-3.5-turbo\n","o1-preview-2024-09-12\n","gpt-3.5-turbo-0125\n","o1-preview\n","gpt-3.5-turbo-instruct\n","babbage-002\n","o1-mini\n","o1-mini-2024-09-12\n","whisper-1\n","dall-e-3\n","omni-moderation-latest\n","omni-moderation-2024-09-26\n","tts-1-hd-1106\n","tts-1-hd\n","davinci-002\n","text-embedding-ada-002\n","tts-1\n","tts-1-1106\n","gpt-3.5-turbo-instruct-0914\n","text-embedding-3-small\n","gpt-3.5-turbo-1106\n","gpt-3.5-turbo-16k\n","gpt-4o-mini\n","gpt-4o-mini-2024-07-18\n","text-embedding-3-large\n"]}]}]}